---
title: 'Text'
description: 'Generate text with chat models using structured inputs and streaming.'
icon: 'message'
---

# Generate Text

<Tip>Generate text by sending structured chat messages to a model.</Tip>

## Quickstart

### Generate a Response

Send a text prompt to generate a response from a chat model.

<CodeGroup>
```javascript javascript
import Bytez from "bytez.js";

(async () => {
  const client = new Bytez("YOUR_BYTEZ_KEY_HERE");
  const model = client.model("microsoft/Phi-3-mini-4k-instruct");

  const messages = [
    {
      role: "system",
      content: "You are a friendly chatbot",
    },
    {
      role: "user",
      content: "What is the capital of England?",
    },
  ];

  const params = { max_length: 100 };

  // Run model and get output
  const { error, output } = await model.run(messages, params);

  if (error) {
    console.error("Error running the model:", error);
    return;
  }

  console.log(output);
})();
```
```python python
from bytez import Bytez

client = Bytez("YOUR_BYTEZ_KEY_HERE")

model = client.model("microsoft/Phi-3-mini-4k-instruct")

model.load()

messages = [
    {"role": "system", "content": "You are a friendly chatbot"},
    {"role": "user", "content": "What is the capital of England?"},
]

result = model.run(messages, model_params={"max_length": 100})

output = result.get("output")

generated_text = output[0]["generated_text"]

for message in generated_text:
    print(message)

    content = message["content"]
    role = message["role"]

    print({"content": content, "role": role})
```


```julia julia
using Bytez

client = Bytez.init("YOUR_BYTEZ_KEY_HERE")

model = client.model("microsoft/Phi-3-mini-4k-instruct")

model.load()

messages = [
	Dict("role" => "system", "content" => "You are a friendly chatbot"),
	Dict("role" => "user", "content" => "What is the capital of England?"),
]

result = model.run(messages, Dict("max_length" => 100))

output = result["output"]

generated_text = output[1]["generated_text"]

for message in generated_text
	println(message)

	content = message["content"]
	role = message["role"]

	println(Dict("content" => content, "role" => role))
end
```

```bash rest
curl --location 'https://api.bytez.com/models/v2/phi-2/model' \
--header 'Authorization: Key YOUR_BYTEZ_KEY_HERE' \
--header 'Content-Type: application/json' \
--data '{
  "messages": [
    {
      "role": "system",
      "content": [
        { "type": "text", "text": "You are a friendly chatbot who responds in the tone of a pirate" }
      ]
    },
    {
      "role": "user",
      "content": [
        { "type": "text", "text": "Why is this video so funny?" },
        { "type": "video", "url": "https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4" }
      ]
    },
    {
      "role": "assistant",
      "content": [
        { "type": "text", "text": "The humor in this video comes from the unexpected and" }
      ]
    }
  ],
  "params": {
    "max_new_tokens": 50
  }
}'
```

</CodeGroup>

## Streaming

Enable real-time text generation by streaming responses.

<CodeGroup>

```python python
import bytez

client = bytez.Bytez("YOUR_BYTEZ_KEY_HERE")
model = client.model("microsoft/Phi-3-mini-4k-instruct")

text_input = [
    {"role": "system", "content": "You are a friendly chatbot."},
    {"role": "user", "content": "How are you?"}
]

params = {"max_new_tokens": 50}

# Enable streaming
stream = model.run(text_input, params, stream=True)

try:
    for chunk in stream:
        print(chunk.decode("utf-8"))  # Process each chunk as it arrives
except Exception as error:
    print(f"Error during streaming: {error}")
```

```javascript javascript
import Bytez from "bytez.js";

const bytez = new Bytez("YOUR_BYTEZ_KEY_HERE");

const textInput = [
  {
    role: "system",
    content: [{ type: "text", text: "You are a friendly chatbot." }]
  },
  {
    role: "user",
    content: [{ type: "text", text: "How are you?" }]
  }
];

const params = { max_new_tokens: 50 };

const model = bytez.model("microsoft/Phi-3-mini-4k-instruct");

// Stream output directly
const stream = await model.run(textInput, params, true);

try {
  // Check if running in a browser
  const textStream = stream.pipeThrough ? stream.pipeThrough(new TextDecoderStream()) : stream;

  for await (const chunk of textStream) {
    console.log(chunk);
  }

  console.log("Streaming ended.");
} catch (error) {
  console.error("Streaming error:", error);
}

```

```julia julia
using Bytez

client = Bytez("YOUR_BYTEZ_KEY_HERE")
model = client.model("microsoft/Phi-3-mini-4k-instruct")

text_input = [
    Dict("role" => "system", "content" => "You are a friendly chatbot."),
    Dict("role" => "user", "content" => "How are you?")
]

params = Dict("max_new_tokens" => 50)

# Enable streaming
stream = model.run(text_input, params, true)

try
    for chunk in stream
        println(String(chunk))  # Print each chunk as it arrives
    end
catch e
    println("Error during streaming: ", e)
end

```

```bash rest
curl --location 'https://api.bytez.com/models/v2/phi-2/model' \
--header 'Authorization: Key YOUR_BYTEZ_KEY_HERE' \
--header 'Content-Type: application/json' \
--data '{
  "messages": [
    {
      "role": "system",
      "content": [
        { "type": "text", "text": "You are a friendly chatbot who responds in the tone of a pirate" }
      ]
    },
    {
      "role": "user",
      "content": [
        { "type": "text", "text": "Why is this video so funny?" },
        { "type": "video", "url": "https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4" }
      ]
    },
    {
      "role": "assistant",
      "content": [
        { "type": "text", "text": "The humor in this video comes from the unexpected and" }
      ]
    }
  ],
  "params": {
    "max_new_tokens": 50,
    "stream": true
  }
}'
```

</CodeGroup>

## Proprietary Models

<Tip>
Our v2 endpoint supports interacting with proprietary models from `Anthropic`, `Google`, `Cohere`, `OpenAI`, and `Mistral`.
</Tip>

### Code

<CodeGroup>

```bash openai
curl --location 'https://api.bytez.com/models/v2/openai/gpt-4o-mini' \
--header 'Authorization: Key YOUR_BYTEZ_KEY_HERE' \
--header 'Provider-Key: PROVIDER_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "messages": [{"role": "user", "content": "Hello my name is Bob and I like to eat"}],
    "stream": false,
    "params": { "max_tokens": 100 }
}'
```
```bash google
Copy code
curl --location 'https://api.bytez.com/models/v2/google/gemini-1.5-flash' \
--header 'Authorization: Key YOUR_BYTEZ_KEY_HERE' \
--header 'Provider-Key: PROVIDER_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "messages": [{"role": "user", "content": "Hello my name is Bob and I like to eat"}],
    "stream": false,
    "params": { "temperature": 1 }
}'
```
```bash cohere
curl --location 'https://api.bytez.com/models/v2/cohere/command-r' \
--header 'Authorization: Key YOUR_BYTEZ_KEY_HERE' \
--header 'Provider-Key: PROVIDER_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "messages": [{"role": "user", "content": "Cats and rabbits who reside in fancy little houses"}],
    "stream": false,
    "params": { "max_tokens": 50 }
}'
```

```bash mistral
curl --location 'https://api.bytez.com/models/v2/mistral/mistral-small-latest' \
--header 'Authorization: Key YOUR_BYTEZ_KEY_HERE' \
--header 'Provider-Key: PROVIDER_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "messages": [{"role": "user", "content": "Cats and rabbits who reside in fancy little houses"}],
    "stream": false,
    "params": { "max_tokens": 50 }
}'
```
```bash anthropic
curl --location 'https://api.bytez.com/models/v2/anthropic/claude-3-haiku-20240307' \
--header 'Authorization: Key YOUR_BYTEZ_KEY_HERE' \
--header 'Provider-Key: {{vault:claude-api-key}}' \
--header 'Content-Type: application/json' \
--data '{
    "messages": [{"role": "user", "content": "Cats and rabbits who reside in fancy little houses"}],
    "stream": false,
    "params": { "max_tokens": 50 }
}'
```
</CodeGroup>

## Demo
<CardGroup>

<Card title="Explore Models" href="/model-api/playground/models" icon="cube">
  Explore 2.6K+ chat models. Find the right model for your use case.
</Card>

<Card title="API Playground" href="model-api/playground/open-source/examples/messages-as-input/chat" icon="webhook">
  Experiment with our API using an example model.
</Card>

</CardGroup>

## Explore Specialized Models

- [`Document Question Answering (DQA)`](../all-tasks/document-question-answering)
- [`Visual Question Answering (VQA)`](../all-tasks/visual-question-answering)
- [`QA Models`](../all-tasks/question-answering)