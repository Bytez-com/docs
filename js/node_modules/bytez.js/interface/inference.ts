export default interface GenerationConfigType {
  /** Stream the response. Default: `false` */
  stream?: boolean;
  /** The maximum length the generated tokens can have. Corresponds to the length of the input prompt + `max_new_tokens`. Its effect is overridden by `max_new_tokens`, if also set. */
  max_length?: number;

  /** The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt. */
  max_new_tokens?: number;

  /** The minimum length of the sequence to be generated. Corresponds to the length of the input prompt + `min_new_tokens`. Its effect is overridden by `min_new_tokens`, if also set. */
  min_length?: number;

  /** The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt. */
  min_new_tokens?: number;

  /** Controls the stopping condition for beam-based methods, like beam-search. */
  early_stopping?: boolean | "never";

  /** The maximum amount of time you allow the computation to run for in seconds. Generation will still finish the current pass after allocated time has been passed. */
  max_time?: number;

  /** Whether or not to use sampling; use greedy decoding otherwise. */
  do_sample?: boolean;

  /** Number of beams for beam search. 1 means no beam search. */
  num_beams?: number;

  /** Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams. */
  num_beam_groups?: number;

  /** The values balance the model confidence and the degeneration penalty in contrastive search decoding. */
  penalty_alpha?: number;

  /** Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding. */
  use_cache?: boolean;

  /** The value used to modulate the next token probabilities. */
  temperature?: number;

  /** The number of highest probability vocabulary tokens to keep for top-k-filtering. */
  top_k?: number;

  /** If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation. */
  top_p?: number;

  /** Local typicality measures how similar the conditional probability of predicting a target token next is to the expected conditional probability of predicting a random token next, given the partial text already generated. */
  typical_p?: number;

  /** If set to float strictly between 0 and 1, only tokens with a conditional probability greater than `epsilon_cutoff` will be sampled. */
  epsilon_cutoff?: number;

  /** Eta sampling is a hybrid of locally typical sampling and epsilon sampling. */
  eta_cutoff?: number;

  /** This value is subtracted from a beam's score if it generates a token same as any beam from other group at a particular time. */
  diversity_penalty?: number;

  /** The parameter for repetition penalty. 1.0 means no penalty. */
  repetition_penalty?: number;

  /** An exponential penalty on sequences that are not in the original input. 1.0 means no penalty. */
  encoder_repetition_penalty?: number;

  /** Exponential penalty to the length that is used with beam-based generation. */
  length_penalty?: number;

  /** If set to int > 0, all ngrams of that size can only occur once. */
  no_repeat_ngram_size?: number;

  /** List of token ids that are not allowed to be generated. */
  bad_words_ids?: number[][];

  /** List of token ids that must be generated. */
  force_words_ids?: number[][] | number[][][];

  /** Whether to renormalize the logits after applying all the logits processors or warpers. */
  renormalize_logits?: boolean;

  /** Custom constraints that can be added to the generation to ensure that the output will contain the use of certain tokens as defined by `Constraint` objects. */
  constraints?: Object[];

  /** The id of the token to force as the first generated token after the `decoder_start_token_id`. */
  forced_bos_token_id?: number;

  /** The id of the token to force as the last generated token when `max_length` is reached. Optionally, use a list to set multiple *end-of-sequence* tokens. */
  forced_eos_token_id?: number | number[];

  /** Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to crash. */
  remove_invalid_values?: boolean;

  /** This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been generated. */
  exponential_decay_length_penalty?: number[];

  /** A list of tokens that will be suppressed at generation. */
  suppress_tokens?: number[];

  /** A list of tokens that will be suppressed at the beginning of the generation. */
  begin_suppress_tokens?: number[];

  /** A list of pairs of integers which indicates a mapping from generation indices to token indices that will be forced before sampling. */
  forced_decoder_ids?: number[][];

  /** The number of independently computed returned sequences for each element in the batch. */
  num_return_sequences?: number;

  /** Whether or not to return the attentions tensors of all attention layers. */
  output_attentions?: boolean;

  /** Whether or not to return the hidden states of all layers. */
  output_hidden_states?: boolean;

  /** Whether or not to return the prediction scores. */
  output_scores?: boolean;

  /** Whether or not to return a `ModelOutput` instead of a plain tuple. */
  return_dict_in_generate?: boolean;

  /** The id of the *padding* token. */
  pad_token_id?: number;

  /** The id of the *beginning-of-sequence* token. */
  bos_token_id?: number;

  /** The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens. */
  eos_token_id?: number | number[];

  /** If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the `decoder_input_ids`. */
  encoder_no_repeat_ngram_size?: number;

  /** If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token. */
  decoder_start_token_id?: number;

  /** Additional generation kwargs will be forwarded to the `generate` function of the model. Kwargs that are not present in `generate`'s signature will be used in the model forward pass. */
  generation_kwargs?: Object;
}
